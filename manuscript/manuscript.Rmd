---
title: |
 Towards Explainability of Machine Learning Models in Insurance Pricing
type: WIP DRAFT 
author:
  - name: Author One
    affil: a
    email: foo@bar.foo
  - name: Author Two
    affil: b
    email: bar@foo.bar
affiliation:
  - num: a
    address: |
      Foo
  - num: b
    address: |
      Bar
bibliography: explain-pricing.bib
appendix: appendix.tex
abstract: |
  Abstract tbd
keywords: |
  actuarial science; general insurance
header-includes: |
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
output: rticles::tf_article
---

# Introduction

Risk classification for property & casualty (P&C) insurance rating has traditionally been done with one-way, or univariate, analysis techniques. In recent years, many insurers have moved towards using generalized linear models (GLM), a multivariate predictive modeling technique, which addresses many shortcomings of univariate approaches, and is currently considered the gold standard in insurance risk classification. At the same time, machine learning (ML) techniques such as deep neural networks have gained popularity in many industries due to their superior predictive performance over linear models [@lecunDeepLearning2015]. In fact, there is a fast growing body of literature on applying ML to P&C reserving [@kuoDeepTriangleDeep2018; @wuthrichMachineLearning2018; @gabrielliNeuralNetwork2019a; @gabrielliNeuralNetwork2019]. However, these ML techniques, often considered to be completely “black box”, have been less successful in gaining adoption in pricing, which is a regulated discipline and requires a certain amount of transparency in models.

If insurers can gain more insight into how ML models behave in risk classification contexts, it would increase their ability to reassure regulators and the public that accepted ratemaking principles are met. Being able to charge more accurate premiums would, in turn, make the risk transfer system more efficient and contribute to the betterment of society. In this paper, we aim to take a step towards liberating actuaries from the confines of linear models in pricing projects, by proposing a framework for explaining ML models for ratemaking that regulators, practitioners, and researchers in actuarial science can build upon.

The rest of this paper is organized as follows: Section \ref{ratemaking} provides an overview of P&C ratemaking, Section \ref{need} discusses the importance of interpretation, Section \ref{interpretability} discusses model interpretability in the context of ratemaking and proposes specific tasks for model explanation, Section \ref{application} describes current model interpretation techniques and applies them to the tasks defined in the previous section, and Section \ref{conclusion} concludes.

# Property and Casualty Ratemaking {#ratemaking}

## History of Ratemaking

Early classification ratemaking procedures were typically univariate in nature. For example, [@lange_1966] notes that (at that time) most major lines of insurance used univariate methods based around the same principle: distributing an overall indication to territorial relativities or classification relativities based on the extent to which they deviated from the average experience.

[@bailey_simon_1960] introduced minimum bias methods, which were expanded throughout the 60s, 70s, and 80s. As computing power developed, minimum bias began to give away to generalized linear models, with papers such as [@brown_1988] and [@mildenhall_1999] bridging the gap between the methods.

Arguably, generalized linear models predate minimum bias procedures by a significant margin. The term "Generalized Linear Model" was coined by [@nelder_wedderburn_1972], but generalizations of least squares linear regression date back at least to the 1930s. Like minimum bias methods, GLMs did not become mainstream in actuarial science for some time. For example, the syllabus of basic education does not seem to include any mention of GLMs prior to [@brown_1988] in the 1990 syllabus for basic education. From there, GLMs seem to have received only passing mention until 2006 with the introduction of [@anderson_2005] to the syllabus.

## Machine Learning in Ratemaking

Paralelling the development of generalized linear models was the development of machine learning algorithms throughout the middle part of the 20th century. Detailed histories of machine learning may be found in sources such as [@Nilsson_2009] and [@wang_raj_2017]. Consistent with GLMs, machine learning was relatively unpopular in actuarial science until the last ten years as computing power has become cheaper and more easily available and as machine learning software packages have obviated the need for developing analyses from scratch each time an analysis is performed. Due to the breadth of machine learning as a field, it is difficult to identify the first time it entered the CAS syllabus; however, cluster analysis (in the form of k-means) seems to have been first included in 2011 with [@robertson_2009]. More recently, the MAS-I and MAS-II exams introduced in 2018 have included machine learning explicitly.

Within the area of ratemaking, machine learning is still in its infancy. A significant portion of machine learning applications to ratemaking has been in the context of automobile telematics, such as [@gao_2018], [@gao_2018_2], [@gao_2019], [@roel_2018], or [@wuthrich_2017]. Presumably this focus has been a result of the high-dimensionality and complexity of telematics data, making it a field in which the unique abilities of machine learning techniques give a clear advantage over traditional approaches.

Outside of telematics, [@yang_2018] uses a gradient tree-boosting approach to capture non-linearities that would be a challenge for GLMs. [@henckaerts_2018] makes use of "generalized additive models" to improve predictions of GLMs. Many researchers, in an apparent effort to demonstrate the range of possibilities and advantages of machine learning, have approached the topic by comparing many different machine learning algorithms within a single study, such as in [@dugas_2003], [@noll_2018], [@spedicato_2018]. These studies make use of such varied techniques as regression trees, boosting machines, support vector machines, and neural networks.

## Ratemaking Process

Regardless of the method employed for determining this risk of various classifications, the actual process of setting rate relativities typically involves some variation of the following steps:

1. Obtain relevant policy-level data
2. Prepare data for analysis
3. Perform analysis on the data, employing desired method or methods to estimate needed rate relativites
4. Select final rate relativities based on rate indications
5. Present rates to the reglator, including explanation of the steps followed to derive the rates
6. Answer questions from regulators regarding the method employed

The focus of this paper is on steps 5 and 6. In particular, rate regulators are concerned with whether rates are inadequate, excessive, or unfairly discriminatory. In many states, rate filings that exceed certain thresholds for magnitude of rate changes or filings that make use of new or sophisticated predictive models may be subject to intense regulatory scrutiny. In these cases, it is necessary to be able to explain the results of the modeling process in a way that is understandable without sacrificing statistical rigor.

It should be noted that communicating results is not simply a method of passing regulatory muster. Generating interpretable modeling output is an important - even essential - facet of model checking. Therefore, the techniques discussed in this paper may be viewed from the lens of providing useful information to regulators, but they should also be considered as part of a thorough vetting of any rating model.

# The Need to Understand the Black Box {#need}

Within the actuarial profession, Actuarial Standard of Practice 41 ("Actuarial Communications") notes that "...another actuary qualified in the same practice area [should be able to] make an objective appraisal of the reasonableness of the actuary's work as presented in the actuarial report." [@asop_41] Underlying this requirement is an assumption that the hypothetical other actuary qualified in the same practice area is adequately familiar with the relevant techniques employed. Although the syllabus of basic education is constantly changing, there has at times been an assumption that all techniques and assumptions that have ever been a part of the syllabus of basic education needn't be explained from first principles in general actuarial communications, and that an actuary practicing in the same field should be able to make an objective appraisal of the results from the methods found in the syllabus. [This can be supported by reference to the edits to ASOP 38 over time - originally it was designed to be about all models, but they revised it to be about only models that incorporate specialized knowledge outside of the actuary's area of expertise... do we need this citation, though?] This is notable because, beginning with the introduction of the MAS-I and MAS-II examination in July of 2018, several machine learning models were formally included in the syllabus of basic education. These exams cover a wide range of topics, such as splines, clustering algorithms, decision trees, boosting, and principle components analysis. [@cas_syllabus_2018]

Nevertheless, machine learning poses something of a special challenge for ASOP 41 for several reasons. Machine learning models can be very ad hoc compared to traditional statistical models. Because many machine learning models are deterministic, they may not admit of standard metrics for model comparison (e.g., it's not straightforward to calculate an AIC over a neural network). In addition, machine learning methods are often combined into ensembles that may not be easily separated and that may, as a collection, cease to resemble a single standard version of a model. Complicating matters still further, machine learning models can be "black boxes" insofar as the final form of response curve cannot be easily predicted and may depend heavily on the available data (which may not, in turn, be available to the reviewer).

This last item raises a final interesting issue. Generalized linear models and their ilk are often fitted using one of a handful of standard and well-understood approaches (e.g., maximum likelihood estimation). However, this is not possible in general with machine learning models, as machine learning algorithms often use loss surfaces that are very complex such that it may not be feasible to calculate the global minimum of the surface. Certainly, closed form representations of the loss surfaces are not generally available. For this reason, the training phase of a machine learning model is, in many ways, just as important to one's understanding as the model form and the data on which the model is fitted. Because the final model result is inseparable from these three components (training method, model form, and data), it is not generally adequate to just know the method employed to make an objective appraisal of the reasonableness of the model. More information is necessary.

Of course, these comments only apply to the actuarial profession. Outside of the actuarial profession, communication of results may be more challenging. A 2017 survey conducted by the Casualty Actuarial and Statistical Task Force of the National Association of Insurance Commissioners found that the plurality of responding regulators identified "Filing complexity and/or a lack of resources or expertise" as a key challenge that impedes their ability to review GLMs or other predictive models. [@naic_summer_2017] Given that machine learning algorithms are generally regarded as more complex than GLMs, this implies that the challenge of communicating machine learning model results is significant.

In response to the same survey, 33 state regulators noted that it would be helpful or very helpful for the NAIC to develop information and tools to assist in reviewing rate filings based on GLMs, and 34 noted that it would be helpful to develop similar items to assist in reviewing "Other Advanced Modeling Techniques." One outgrowth of this need was the development of a white paper on best practices for regulatory review of predictive models. The white paper focuses on review of GLMs, particularly with respect to private passenger automobile and homeowners' insurance. Some of the guidance offered in this regard is therefore not strictly applicable to the review of machine learning models. For example, as previously noted, p-values are not a concept that translates well to deterministic machine learning algorithms. However, among the guidance applicable to machine learning algorithms are the following (paraphrasing):

* understand the relationship between the inputs and the expected loss or expense differences in risk,
* determine that these input characteristics are not unfairly discriminatory, and
* determine the extent of premium disruption among policyholders and be able to explain the source of premium disruptions. [@naic_white_paper]

Note that this list is not exhaustive of the guidance offered by any means, but that these three items are those that may present greater challenge for machine learning algorithms compared to GLMs. 

Depending on the model, machine learning algorithms can be non-linear in the inputs. In areas where data are sparse, the model could generate unnatural response curves that could go undetected by a very high-level view of the model results. This leads to a situation where lift charts or tests on holdout data may indicate that the model is performing adequately, but it may be difficult to explain the reasons for a particular premium indicaton or a change in premium when policy characteristics change.

-For this, we recommend ____ (grid search?)
-Because the model form may be difficult to represent in the form of a single equation (and harder still to evaluate), one method of evaluating the reasonableness of model results is local approximations of the impact of different rating variables (LIME?)

# Interpretability in the Ratemaking Context {#interpretability}

In this section, we attempt to develop a working definition of interpretability for ratemaking applications. While we will not provide a comprehensive survey of the prolific and fast evolving ML interpretability literature, we draw from it as appropriate in setting the stage for our discussion. There is not a standard definition of interpretability with consensus from all researchers; here are a few from frequently cited papers:

- Ability to explain or to present in understandable terms to a human [@doshi-velezRigorousScience2017];
- The degree to which an observer can understand the cause of a decision [@biranExplanationJustification2017]; and
- A method is interpretable if a user can correctly and efficiently predict the method’s results [@kimExamplesAre2016]. 

We motivate our discussion by considering several aspects of interpretability. For the time being, one can take any of the definitions cited above. As we proceed through the points below, we aim to arrive at a more scoped and relevant definition of what it means to attain interpretability in a pricing model. (TODO: add a sentence mapping out what we'll talk about in this section)

## Not All Linear Models are Interpretable

We look at some examples of models that are considered by many to be interpretable. The GLM is probably the most oft-cited example of such a model (or rather, modeling framework) in the actuarial literature. Given a set of inputs, we can easily reason about what the output of the model is. As an illustrative example, we may have a claims severity model with age, amount of insurance (AOI), and gender as features; assuming a log link function, we have

\begin{equation}
  \log(E(\text{severity})) = \beta_0 + \beta_1 \cdot \text{age} + \beta_2 \cdot \text{AOI} + \beta_3 \cdot \text{gender}_{\text{male}}.
\end{equation}

Here, we can tell, for example, what the model would predict for the expected severity if we were to increase age by a certain amount, *all else being equal*, because the relationship between the feature and the response is simply the coefficient $\beta_1$ and the link function. Another commonly cited example of an interpretable model is a decision tree. An illustrative example is shown in (TODO: FIGURE). Here, the prediction is arrived at by following a sequence of if-else decisions. 

Now, it is worth pointing out that, when declaring that GLM or decision trees are interpretable models, we are implicitly qualifying that we are considering a reasonable number of linear features. In fact, the ease at which we can reason about a model rapidly declines as the number of features and interactions increase, as in the following (somewhat pathological) example:

\begin{equation}\label{eq:badglm}
  \log(E(\text{severity})) = \beta_0 + \beta_1 \cdot \text{AOI}  + \beta_2\cdot\text{age} + \beta_3\cdot\text{age}^2 + \beta_4 \cdot \text{age:AOI} + \beta_5 \cdot \text{gender}_{\text{male}}.
\end{equation}

Similarly, one can see that in (TODO: FIGURE), large and deep trees are tough to reason about. In other words, even when working within the framework of an "interpretable" class of models, we may still end up with something that many would consider "black box."

## The Machinery is Not a Secret

An occasional misconception is that we have no visibility into how some ML models *compute* predictions, which render them uninterpretable. Outside of proprietary algorithms, all common ML models, including neural networks, gradient boosted trees, and random forests, are well studied and have large bodies of literature documenting their inner workings. As an example, a fitted neural network model on tabular data is simply a composition of linear transformations followed with nonlinear activation functions. As in Equation \ref{eq:badglm}, one can write down the mathematical equation for calculating the prediction given some inputs, but it may be difficult for a human to reason about it. This is a somewhat philosophical point, as we show later in the paper that we can still provide explanations of completely "black box" models, but is important to note given that pricing models undergo regulatory scrutiny.

## Explanations are Social

Proposed by @hiltonConversationalProcesses1990 and interpreted by @millerExplanationArtificial2017 in the context of ML, model explanations are conversations or social interactions. One consequence of this identification is that explanations need to be *relevant* to the *audience*. In developing, filing, and operationalizing a pricing model, one needs to accommodate a variety of stakeholders, each of whom has a different set of questions, assumptions, and technical capacity. First, there are internal stakeholders at the company, which includes management and underwriters. While some of the individuals in this audience may be technical, they are likely less familiar with predictive modeling techniques than the actuaries and data scientists who build the models. (TODO: what's a representative question here?) Next, we have the regulators, who may have limited resources to review the models, and will focus on a specific list of questions. 


# Applying Model Interpretation Techniques {#application}

(some definitions, e.g. global/model vs. local/instance, categorize questions accordingly)

## (answer each question)

(for each question, propose a technique, mention alternative techniques, pros/cons, implement, interpret results)

# Conclusion {#conclusion}

(conclude)
